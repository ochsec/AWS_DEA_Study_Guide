[Table of Contents](../../README.md) | [1.2 Apply data transformation and cleansing techniques ->](./1.2_Apply_data_transformation_and_cleansing_techniques.md)
---
# Domain 1: Data Ingestion and Transformation
## Task 1.1: Ingest data from various sources

This section covers the methods and AWS services used to ingest data from diverse sources into the AWS ecosystem. Understanding these concepts is crucial for building robust data pipelines.

### Key Concepts

*   **Batch Ingestion:** Processing data in large blocks at scheduled intervals. Suitable for scenarios where real-time processing isn't required, like daily reporting or historical analysis.
*   **Streaming Ingestion:** Processing data continuously as it arrives, typically in near real-time. Essential for applications like fraud detection, real-time analytics, and IoT data processing.
*   **Data Sources:**
    *   **Databases:** Relational (RDS, Aurora, on-premises SQL Server/Oracle/PostgreSQL/MySQL) and NoSQL (DynamoDB, MongoDB).
    *   **Streaming Data:** IoT devices, application logs, clickstreams, social media feeds.
    *   **Data Lakes:** Existing data stored in S3 or other storage systems.
    *   **APIs:** Data exposed via RESTful or other API endpoints (SaaS applications, internal microservices).
    *   **Files:** Flat files (CSV, JSON, TXT), semi-structured (XML), or structured (Parquet, Avro, ORC) stored in S3, SFTP servers, or local file systems.
*   **Data Formats:** Common formats include CSV, JSON, XML, Parquet, ORC, Avro. The choice depends on the source, processing requirements, and storage efficiency.
*   **Connection Methods:** Direct database connections (JDBC/ODBC), API calls, SDKs, agents, file transfers (SFTP/FTP), direct network connections (Direct Connect).

### Relevant AWS Services

**1. Amazon Kinesis**
A suite of services for collecting, processing, and analyzing real-time streaming data.

*   **Kinesis Data Streams:**
    *   **Purpose:** Ingest and store large streams of data records in real-time. Highly scalable and durable.
    *   **Use Cases:** Real-time analytics dashboards, log and event data collection, clickstream analysis.
    *   **Concepts:** Shards (base unit of throughput), Producers (send data), Consumers (read data - EC2 apps, Lambda, Kinesis Data Analytics, Kinesis Data Firehose). Data retention up to 365 days.
*   **Kinesis Data Firehose:**
    *   **Purpose:** Reliably load streaming data into data lakes, data stores, and analytics services. Simplifies ingestion without needing to manage streams or write consumer applications.
    *   **Use Cases:** Loading streaming data into S3, Redshift, OpenSearch Service, Splunk, or custom HTTP endpoints.
    *   **Concepts:** Buffering (time/size), data transformation (Lambda integration), data format conversion (e.g., JSON to Parquet/ORC), compression (Gzip, Snappy, ZIP), error logging.
*   **Kinesis Video Streams:**
    *   **Purpose:** Securely stream video from connected devices to AWS for analytics, machine learning (ML), playback, and other processing.
    *   **Use Cases:** Smart home devices, industrial monitoring, public safety cameras.
*   **Kinesis Data Analytics:**
    *   **Purpose:** Process and analyze streaming data in real-time using SQL or Apache Flink. (Also relevant to Task 1.2).
    *   **Use Cases:** Real-time dashboards, time-series analytics, anomaly detection.

**2. AWS Database Migration Service (DMS)**
*   **Purpose:** Migrate databases to AWS easily and securely. Supports homogeneous (e.g., Oracle to Oracle on RDS) and heterogeneous (e.g., SQL Server to Aurora PostgreSQL) migrations. Also enables continuous data replication (Change Data Capture - CDC).
*   **Use Cases:** Migrating on-premises databases to RDS/Aurora/Redshift/S3/DynamoDB, continuous replication for data warehousing or disaster recovery.
*   **Concepts:** Source Endpoint, Target Endpoint, Replication Instance, Migration Task (Full Load, CDC, Full Load + CDC). Schema Conversion Tool (SCT) often used alongside DMS for heterogeneous migrations.

**3. AWS Glue**
*   **Purpose:** A fully managed extract, transform, and load (ETL) service. While primarily for ETL (Task 1.2), its crawlers and connections are relevant for ingestion.
*   **Use Cases:** Discovering data schemas (Crawlers), connecting to various data sources (JDBC, S3, DynamoDB), loading data into data lakes/warehouses.
*   **Concepts:** Data Catalog (metadata repository), Crawlers (infer schema and populate Data Catalog), Connections (store credentials for data sources).

**4. Amazon S3 (Simple Storage Service)**
*   **Purpose:** Object storage service offering industry-leading scalability, data availability, security, and performance. Often the central component of a data lake and a common target/source for ingestion.
*   **Use Cases:** Data lake foundation, staging area for ETL, backup and restore, log storage.
*   **Concepts:** Buckets, Objects, Prefixes, Storage Classes (Standard, Intelligent-Tiering, Glacier), Lifecycle Policies, Event Notifications (trigger Lambda/SQS/SNS on object creation).

**5. Amazon API Gateway**
*   **Purpose:** Create, publish, maintain, monitor, and secure APIs at any scale. Can act as an ingestion point for data sent via HTTP requests.
*   **Use Cases:** Exposing data ingestion endpoints for web/mobile apps, integrating with backend services (Lambda, Step Functions, Kinesis).
*   **Concepts:** RESTful APIs, WebSocket APIs, HTTP APIs, Stages, Throttling, Caching, Authorization (IAM, Cognito, Lambda Authorizers).

**6. Amazon AppFlow**
*   **Purpose:** Fully managed integration service enabling secure data transfer between SaaS applications (like Salesforce, Slack, Marketo) and AWS services (like S3, Redshift).
*   **Use Cases:** Ingesting customer data from CRMs, synchronizing marketing data, aggregating operational data from various SaaS tools.
*   **Concepts:** Flows (define source, destination, triggers, mappings, transformations), Connectors. No-code interface.

**7. AWS Snow Family**
*   **Purpose:** Services for moving large amounts of data into and out of AWS when network transfer is impractical due to time, cost, or bandwidth constraints.
*   **Use Cases:** Migrating petabyte-scale data, data center decommissioning, disaster recovery, remote data collection.
*   **Services:**
    *   **Snowcone:** Smallest device (8TB/14TB), portable, rugged, includes compute capabilities (EC2, IoT Greengrass).
    *   **Snowball Edge:** Larger capacity (Storage Optimized: 80TB usable, Compute Optimized: more vCPUs/memory), includes compute.
    *   **Snowmobile:** Exabyte-scale data transfer service using a 45-foot ruggedized shipping container.

**8. Other Relevant Services**
*   **AWS Transfer Family:** Fully managed support for file transfers directly into and out of Amazon S3 or Amazon EFS using SFTP, FTPS, and FTP.
*   **AWS DataSync:** Online data transfer service simplifying, automating, and accelerating moving data between on-premises storage systems and AWS Storage services (S3, EFS, FSx), as well as between AWS storage services.
*   **AWS IoT Core:** Managed cloud service letting connected devices easily and securely interact with cloud applications and other devices. Ingests data from IoT devices.

### Choosing the Right Ingestion Service

| Scenario                      | Primary Service(s)                                  | Key Considerations                                     |
| :---------------------------- | :-------------------------------------------------- | :----------------------------------------------------- |
| Real-time application logs    | Kinesis Data Streams, Kinesis Data Firehose         | Latency requirements, downstream processing needs      |
| IoT sensor data               | IoT Core, Kinesis Data Streams/Firehose             | Device management, protocol support (MQTT), scale      |
| Clickstream data              | Kinesis Data Streams/Firehose, API Gateway + Lambda | Volume, velocity, real-time analysis needs           |
| Database Migration (One-time) | DMS, Snowball (if large)                            | Downtime tolerance, database size, network bandwidth |
| Database Replication (Ongoing)| DMS (CDC)                                           | Source/Target compatibility, latency, transformation |
| Batch files from SFTP         | Transfer Family, DataSync, Custom EC2/Lambda script | Frequency, file size, security, automation needs     |
| Data from SaaS applications   | AppFlow, Custom API integration (API Gateway/Lambda)| Available connectors, transformation needs, ease of use |
| Large dataset transfer        | Snowball Edge, Snowcone, DataSync, Direct Connect   | Data volume, network bandwidth, time constraints     |
| API-based data ingestion      | API Gateway + (Lambda/Kinesis/Step Functions)       | Request volume, security, backend processing logic   |

### Summary

Effective data ingestion requires understanding the characteristics of the source data (volume, velocity, variety, veracity) and the requirements of the downstream processing and analytics. AWS provides a comprehensive set of tools to handle diverse ingestion scenarios, from real-time streaming data to large-scale batch transfers and SaaS integrations. Choosing the right combination of services is key to building efficient, scalable, and cost-effective data pipelines.