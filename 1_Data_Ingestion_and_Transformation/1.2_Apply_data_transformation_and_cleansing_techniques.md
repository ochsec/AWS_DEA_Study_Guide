[<- 1.1 Ingest data from various sources](./1.1_Ingest_data_from_various_sources.md) | [Table of Contents](../../README.md) | [1.3 Orchestrate data pipelines ->](./1.3_Orchestrate_data_pipelines.md)
---
# Domain 1: Data Ingestion and Transformation
## Task 1.2: Apply data transformation and cleansing techniques

Once data is ingested, it often needs to be transformed and cleansed to make it suitable for analysis, machine learning, or storage in a structured format. This task covers the techniques and AWS services used for this critical step in the data pipeline.

### Key Concepts

*   **Data Transformation:** Converting data from its original format or structure into a desired format or structure. Examples include changing data types, joining datasets, aggregating data, and pivoting data.
*   **Data Cleansing:** Identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Examples include handling missing values, removing duplicates, standardizing formats (e.g., dates, addresses), and validating data against predefined rules.
*   **Data Validation:** Ensuring data conforms to specific quality rules and standards.
*   **Normalization:** Organizing data to minimize redundancy (often in relational databases, but concepts apply elsewhere).
*   **Denormalization:** Intentionally introducing redundancy to improve query performance (common in data warehousing).
*   **Data Enrichment:** Augmenting data by adding information from other sources (e.g., adding demographic data based on zip codes).
*   **Partitioning:** Dividing large datasets into smaller, more manageable parts based on specific column values (e.g., year, month, country). Improves query performance and cost efficiency by reducing the amount of data scanned.
*   **Compression:** Reducing the physical size of data to save storage space and improve query performance (less data to read/transfer). Common codecs include Snappy, Gzip, Bzip2, LZO.
*   **File Formats:**
    *   **Row-based (e.g., CSV, JSON, Avro):** Good for write-heavy workloads or when entire rows need processing. Avro includes schema information.
    *   **Columnar (e.g., Parquet, ORC):** Optimized for read-heavy analytical queries. Store data by column, allowing queries to read only the necessary columns. Offer excellent compression and predicate pushdown capabilities. Parquet is widely used in the Hadoop/Spark ecosystem; ORC is also common, particularly with Hive.

### Relevant AWS Services

**1. AWS Glue**
A central service for data transformation and cleansing in AWS.

*   **Glue ETL Jobs:**
    *   **Purpose:** Run serverless Spark or Python shell scripts to perform complex ETL tasks. Auto-scales resources based on workload.
    *   **Use Cases:** Transforming large datasets, joining data from multiple sources, applying complex business logic, loading data into data warehouses/lakes.
    *   **Concepts:** Scripts (generated or custom PySpark/Scala/Python), Development Endpoints (interactive development), Job Bookmarks (process only new data), Workflows (orchestrate multiple jobs - see Task 1.3). Supports various sources/targets (S3, JDBC, DynamoDB, etc.).
*   **Glue DataBrew:**
    *   **Purpose:** Visual data preparation tool enabling users (data analysts, data scientists) to clean and normalize data without writing code.
    *   **Use Cases:** Quickly cleaning and transforming datasets for analysis or ML, standardizing formats, filtering bad data.
    *   **Concepts:** Projects, Datasets, Recipes (series of transformation steps), over 250 built-in transformations. Output to S3, Redshift, RDS, etc.
*   **Glue Crawlers:**
    *   **Purpose:** Automatically discover datasets, infer schemas, and populate the AWS Glue Data Catalog. Can detect schema changes. (Also relevant to Task 1.1).
    *   **Use Cases:** Cataloging data in S3 or JDBC stores, keeping schemas up-to-date.
*   **Glue Schema Registry:**
    *   **Purpose:** Central repository to store, discover, and evolve schemas. Integrates with Kinesis Data Streams, Kinesis Data Analytics, Lambda, and Glue ETL jobs for schema validation and evolution during streaming ingestion and processing.
    *   **Use Cases:** Enforcing schema compliance for streaming data producers and consumers, managing schema evolution over time.

**2. Amazon EMR (Elastic MapReduce)**
*   **Purpose:** Managed cluster platform simplifying running big data frameworks like Apache Spark, Hadoop, Hive, Presto, Flink, etc., to process and analyze vast amounts of data. Offers more control over the environment than Glue ETL.
*   **Use Cases:** Large-scale batch ETL, machine learning, interactive SQL (Presto/Hive), streaming data processing (Spark Streaming/Flink).
*   **Concepts:** Clusters (EC2 instances), Steps (units of work), Bootstrap Actions (customize nodes on startup), Instance Fleets/Groups, EMR Studio (IDE), EMR Serverless (automatically provisions and scales resources).

**3. AWS Lambda**
*   **Purpose:** Serverless compute service running code in response to events. Can be used for lightweight transformations, especially for streaming data or event-driven pipelines.
*   **Use Cases:** Transforming data records from Kinesis Data Streams or DynamoDB Streams, enriching data via API calls, validating data upon arrival in S3 (via S3 Event Notifications).
*   **Concepts:** Event Sources (S3, Kinesis, API Gateway, etc.), Triggers, Function Code (Python, Node.js, Java, etc.), Concurrency, Memory/Timeout limits. Best suited for short-running, stateless transformations.

**4. AWS Step Functions**
*   **Purpose:** Serverless function orchestrator to build resilient workflows using visual diagrams. While primarily for orchestration (Task 1.3), it can coordinate transformation steps involving Lambda, Glue, EMR, etc.
*   **Use Cases:** Building multi-step transformation pipelines, coordinating Lambda functions for complex logic, handling errors and retries in transformations.
*   **Concepts:** State Machines, States (Task, Choice, Parallel, Map, etc.), Integrations with various AWS services.

**5. Amazon Kinesis Data Analytics**
*   **Purpose:** Process and analyze streaming data in real-time using SQL or Apache Flink. (Also relevant to Task 1.1).
*   **Use Cases:** Real-time data cleansing and transformation, filtering streaming data, aggregating data over time windows.
*   **Concepts:** SQL Applications (simpler, SQL-based), Apache Flink Applications (more complex, Java/Scala/Python), Sources (Kinesis Data Streams, Firehose), Destinations (Kinesis Data Streams, Firehose, Lambda).

**6. Amazon Athena**
*   **Purpose:** Interactive query service making it easy to analyze data directly in Amazon S3 using standard SQL. Serverless, pay per query. Can be used for *ad-hoc* transformation or validation.
*   **Use Cases:** Querying data in S3 for exploration, validating data after transformation, creating transformed views or tables (CTAS - Create Table As Select).
*   **Concepts:** Uses Glue Data Catalog for schemas, supports various data formats (CSV, JSON, ORC, Parquet, Avro), integrates with QuickSight for visualization.

### Transformation and Cleansing Techniques in Practice

*   **Handling Missing Values:** Impute (mean, median, mode, specific value), remove rows/columns, use algorithms robust to missing data. (Glue DataBrew, Glue ETL, EMR, Lambda).
*   **Removing Duplicates:** Identify duplicates based on key columns and remove them. (Glue ETL `dropDuplicates()`, EMR Spark `dropDuplicates()`, SQL `GROUP BY` or `ROW_NUMBER()` in Athena/Redshift).
*   **Standardizing Formats:** Convert dates/times to consistent formats, parse/standardize addresses, enforce consistent casing. (Glue DataBrew transformations, Glue ETL/EMR Spark functions, Lambda).
*   **Data Type Conversion:** Change strings to numbers, numbers to dates, etc. (Glue ETL/EMR Spark `cast()`, Glue DataBrew, SQL `CAST`/`CONVERT`).
*   **Joining Datasets:** Combine data from multiple sources based on common keys. (Glue ETL/EMR Spark joins, SQL `JOIN` in Athena/Redshift).
*   **Filtering:** Remove rows that don't meet certain criteria. (Glue ETL/EMR Spark `filter()`, Glue DataBrew, SQL `WHERE`).
*   **Applying Business Rules:** Implement custom logic for validation or transformation. (Glue ETL, EMR, Lambda).
*   **Partitioning Data (Output):** Write transformed data to S3 using partitioning schemes (e.g., `s3://bucket/data/year=2023/month=12/`). (Glue ETL, EMR Spark `partitionBy()`).
*   **Choosing Output Format/Compression:** Select appropriate formats (Parquet/ORC for analytics) and compression (Snappy/Gzip) for the target use case. (Glue ETL, EMR Spark write options).

### Choosing the Right Transformation Service

| Scenario                                  | Primary Service(s)        | Key Considerations                                                     |
| :---------------------------------------- | :------------------------ | :--------------------------------------------------------------------- |
| Visual data prep, no code                 | Glue DataBrew             | User skill set (analysts), complexity of transformations, dataset size |
| Serverless Spark/Python ETL               | Glue ETL Jobs             | Scalability needs, preference for managed Spark, integration with Glue Catalog |
| Full control over big data frameworks     | EMR                       | Need for specific framework versions, custom libraries, cost optimization |
| Lightweight, event-driven transformations | Lambda                    | Latency requirements, stateless operations, integration with event sources |
| Real-time stream transformation/cleansing | Kinesis Data Analytics    | Streaming source, SQL vs Flink preference, real-time needs             |
| Ad-hoc transformation/validation via SQL  | Athena (CTAS)             | Data resides in S3, SQL familiarity, interactive exploration needs     |
| Orchestrating multiple transformation steps | Step Functions            | Complex workflows, error handling, coordinating different services       |
| Schema enforcement for streaming data     | Glue Schema Registry      | Kafka/Kinesis integration, managing schema evolution                   |

### Summary

Data transformation and cleansing are essential for ensuring data quality and usability. AWS offers a range of services catering to different needs, from visual tools like Glue DataBrew to powerful processing engines like Glue ETL and EMR, and real-time processors like Kinesis Data Analytics. Choosing the right service depends on factors like data volume, complexity, required latency, user skills, and the desired level of control. Effective use of partitioning, compression, and appropriate file formats (especially columnar formats like Parquet/ORC) is crucial for optimizing storage and query performance in data lakes and warehouses.