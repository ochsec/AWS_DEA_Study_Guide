[<- 2.3 Manage the data lifecycle](../2_Data_Store_Management/2.3_Manage_the_data_lifecycle.md) | [Table of Contents](../../README.md) | [3.2 Monitor data pipelines and data stores ->](./3.2_Monitor_data_pipelines_and_data_stores.md)
---
# Domain 3: Data Operations and Monitoring
## Task 3.1: Implement data quality checks

### Importance of Data Quality

Data quality is crucial for reliable analytics, accurate reporting, and trustworthy machine learning models. Poor data quality can lead to flawed business decisions, operational inefficiencies, and compliance issues. Ensuring data quality involves verifying that data is:

*   **Complete:** Are there missing values where they shouldn't be?
*   **Accurate:** Does the data correctly reflect the real-world entity or event it represents?
*   **Timely:** Is the data available when needed? Is it up-to-date?
*   **Unique:** Are there duplicate records that shouldn't exist?
*   **Consistent:** Does data stored in different locations or tables contradict itself? Is data in the correct format?
*   **Valid:** Does the data conform to defined rules, constraints, or ranges?

### Methods for Implementing Data Quality Checks

Several AWS services and techniques can be used to implement data quality checks within data pipelines.

#### 1. AWS Glue Data Quality

AWS Glue Data Quality is a feature within AWS Glue that automates data quality management for data lakes and pipelines.

*   **Data Quality Definition Language (DQDL):** Define data quality rules using an intuitive, SQL-like syntax.
    *   **Rule Types:** Supports various checks like completeness (`IsComplete`), uniqueness (`IsUnique`), validity (`IsValueInSet`, `IsInRange`), consistency (`IsReferentialIntegrity`), accuracy (`Mean`, `Sum`, `StandardDeviation`), timeliness, and custom SQL expressions.
    *   **Example Rules:**
        ```sql
        -- Check if 'customer_id' is never null
        IsComplete "customer_id"

        -- Check if 'order_id' is unique across the dataset
        IsUnique "order_id"

        -- Check if 'status' column only contains 'Pending', 'Shipped', or 'Delivered'
        IsValueInSet "status" ['Pending', 'Shipped', 'Delivered']

        -- Check if 'order_total' is always greater than 0
        IsGreaterThan "order_total" 0

        -- Custom check for email format
        CustomSql "SELECT COUNT(*) FROM primary WHERE NOT REGEXP_LIKE(email, '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')" -- Expects 0 rows failing the pattern
        ```
*   **Rulesets:** Group multiple rules together to evaluate a dataset.
*   **Evaluation:** Run data quality checks against Glue Data Catalog tables or directly on DynamicFrames within Glue ETL jobs.
*   **Actions:** Define actions to take based on rule outcomes:
    *   **Publish metrics to CloudWatch:** Track quality scores over time.
    *   **Send notifications via SNS:** Alert stakeholders about failures.
    *   **Stop pipeline execution:** Prevent bad data from propagating (e.g., fail the Glue job).
    *   **Quarantine bad records:** Move failing records to a separate location (e.g., S3 bucket) for investigation.
*   **Integration:** Seamlessly integrates with Glue Data Catalog and Glue ETL jobs.

#### 2. Custom Checks with AWS Lambda and AWS Step Functions

For more complex or specialized data quality checks not covered by Glue Data Quality, or when integrating with non-Glue pipelines, you can use Lambda functions orchestrated by Step Functions.

*   **AWS Lambda:** Write custom code (Python, Node.js, etc.) to perform specific validation logic.
    *   Can read data from various sources (S3, DynamoDB, RDS).
    *   Can interact with external validation services or APIs.
    *   Can publish results to CloudWatch Metrics or Logs, or trigger SNS notifications.
*   **AWS Step Functions:** Orchestrate a workflow involving Lambda functions for data quality checks.
    *   Define conditional logic based on validation results (e.g., proceed if checks pass, trigger remediation if they fail).
    *   Integrate checks as steps within a larger data processing pipeline.
    *   Provides visualization and monitoring of the validation process.

#### 3. Integrating Checks into Data Pipelines

Data quality checks should be embedded directly into data pipelines to catch issues early.

*   **AWS Glue ETL Jobs:**
    *   Use the built-in Glue Data Quality transforms (`EvaluateDataQuality`).
    *   Write custom PySpark code within the job to perform validations on DynamicFrames or DataFrames.
    *   Fail the job if critical quality checks fail.
*   **Amazon EMR:**
    *   Use Spark SQL or DataFrame operations within EMR steps to perform checks.
    *   Leverage libraries like Deequ (an open-source library built on Spark for data quality).
    *   Add EMR steps specifically for data validation.
*   **AWS Data Pipeline:** While less common for new deployments, you can incorporate ShellCommandActivity or EmrActivity to run validation scripts.
*   **Orchestration Tools (e.g., Step Functions, Managed Apache Airflow):** Integrate dedicated validation tasks/operators within the overall workflow.

### Key Data Quality Dimensions and Checks

*   **Completeness:**
    *   Check for NULL or empty values in required columns (`IsComplete` in Glue DQ, `COUNT(*) WHERE col IS NULL` in SQL).
*   **Accuracy:**
    *   Compare data against a known source of truth.
    *   Check for values outside expected ranges (`IsInRange` in Glue DQ, `WHERE col < min OR col > max`).
    *   Validate formats (e.g., email, phone number) using regex (`CustomSql` in Glue DQ, `REGEXP_LIKE` in SQL).
    *   Statistical checks (mean, standard deviation) to detect anomalies.
*   **Timeliness:**
    *   Check if data arrival time meets SLAs.
    *   Verify timestamps are within expected ranges or recent.
*   **Uniqueness:**
    *   Check for duplicate primary keys or unique identifiers (`IsUnique` in Glue DQ, `GROUP BY key HAVING COUNT(*) > 1` in SQL).
*   **Consistency:**
    *   Ensure data conforms to expected formats across records/systems.
    *   Check referential integrity between related tables (`IsReferentialIntegrity` in Glue DQ, JOINs in SQL).
    *   Verify that related fields have consistent values (e.g., city, state, zip code).
*   **Validity:**
    *   Check if values belong to a predefined set (`IsValueInSet` in Glue DQ, `WHERE col NOT IN ('A', 'B', 'C')`).
    *   Ensure data types are correct.

Implementing robust data quality checks is an ongoing process that requires understanding the data, defining clear rules, integrating checks into pipelines, and monitoring results to continuously improve data reliability.