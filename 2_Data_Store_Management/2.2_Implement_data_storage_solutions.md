[<- 2.1 Select appropriate data stores](./2.1_Select_appropriate_data_stores.md) | [Table of Contents](../../README.md) | [2.3 Manage the data lifecycle ->](./2.3_Manage_the_data_lifecycle.md)
---
# Domain 2: Data Store Management
## Task 2.2: Implement data storage solutions

Once an appropriate data store is selected, the next step is to implement it effectively. This involves configuration, setup, and applying best practices for data modeling, partitioning, and indexing to ensure performance, scalability, and manageability.

### Implementing S3 Solutions

*   **Bucket Creation:** Choose a unique name, region.
*   **Bucket Policies:** Control access at the bucket level using JSON policies. Define permissions for users, roles, or other AWS services.
    *   *Example:* Granting read-only access to a specific IAM role.
*   **Access Control Lists (ACLs):** Legacy mechanism for fine-grained object-level access control (generally prefer bucket policies or IAM).
*   **Versioning:** Keep multiple versions of an object. Protects against accidental deletions or overwrites.
*   **Lifecycle Rules:** Automate transitioning objects between storage classes (e.g., S3 Standard -> S3 Standard-IA -> S3 Glacier Deep Archive) or expiring objects.
    *   *Example:* Move logs older than 90 days to Glacier, delete logs older than 1 year.
*   **Replication (CRR/SRR):** Copy objects to another bucket in the same (SRR) or different (CRR) region for disaster recovery, compliance, or lower latency access.
*   **S3 Storage Lens:** Organization-wide visibility into object storage usage and activity.
*   **S3 Storage Classes:** Choose based on access frequency and retrieval time needs (Standard, Intelligent-Tiering, Standard-IA, One Zone-IA, Glacier Instant Retrieval, Glacier Flexible Retrieval, Glacier Deep Archive).

### Implementing Redshift Solutions

*   **Cluster Configuration:**
    *   **Node Types:** Select based on compute, memory, and storage needs (e.g., DC2 for compute-intensive, RA3 for scaling compute and storage independently).
    *   **Number of Nodes:** Determines overall cluster capacity and performance.
    *   **Cluster Type:** Provisioned Cluster vs. Serverless (automatically provisions and scales).
*   **Data Loading:** Use `COPY` command (highly parallel) from S3, DynamoDB, EMR, or SSH hosts. Use AWS Glue or other ETL tools.
*   **Schema Design:**
    *   **Distribution Styles (DISTSTYLE):** How table data is distributed across nodes.
        *   `AUTO`: Redshift determines the best style.
        *   `EVEN`: Round-robin distribution (good default for tables not used in joins).
        *   `KEY`: Distribute based on the values in one column (co-locate joining keys).
        *   `ALL`: Copy entire table to every node (good for small dimension tables).
    *   **Sort Keys (SORTKEY):** Order data within each node. Improves query performance by reducing data scanned (zone maps).
        *   `COMPOUND`: Multi-column sort key, effective when filtering on prefixes.
        *   `INTERLEAVED`: Equal weight to each column, better for filtering on non-prefix columns (less common now).
        *   `AUTO`: Redshift determines the best sort key.
    *   **Compression Encodings:** Reduce storage footprint and I/O (e.g., ZSTD, LZO). Use `ANALYZE COMPRESSION` or let Redshift choose (`AUTO`).
*   **Workload Management (WLM):** Define queues to prioritize queries and manage cluster resources.
*   **Concurrency Scaling:** Automatically add temporary cluster capacity to handle read query bursts.
*   **Redshift Spectrum:** Query data directly in S3 data lake without loading it into Redshift.

### Implementing DynamoDB Solutions

*   **Table Design:**
    *   **Primary Key:** Uniquely identifies each item.
        *   **Partition Key (Hash Key):** Determines the partition where the item is stored. Choose high-cardinality attributes for even distribution.
        *   **Sort Key (Range Key) (Optional):** Orders items within a partition. Enables range queries (`BETWEEN`, `>`, `<`).
*   **Capacity Modes:**
    *   **Provisioned Capacity:** Specify Read Capacity Units (RCUs) and Write Capacity Units (WCUs). Predictable costs, requires capacity planning. Use Auto Scaling.
    *   **On-Demand Capacity:** Pay-per-request. Handles unpredictable workloads automatically, potentially higher cost for sustained high traffic.
*   **Indexes:**
    *   **Local Secondary Index (LSI):** Same partition key, different sort key. Created at table creation. Shares table's provisioned capacity. Strong or eventual consistency. Limited total size per partition key value (10 GB).
    *   **Global Secondary Index (GSI):** Different partition key and optional sort key. Created anytime. Has its own provisioned capacity. Eventual consistency only. Enables different access patterns.
*   **Data Types:** Supports scalar (String, Number, Binary, Boolean, Null), document (List, Map), and set types (StringSet, NumberSet, BinarySet).
*   **DynamoDB Streams:** Time-ordered sequence of item-level modifications (creates, updates, deletes). Can trigger Lambda functions for downstream processing.
*   **Time To Live (TTL):** Automatically delete expired items based on a timestamp attribute. Useful for session data, logs.
*   **Global Tables:** Multi-region, multi-active replication for high availability and low-latency access across geographies.

### Implementing RDS/Aurora Solutions

*   **Instance Types:** Choose based on vCPU, memory, network performance (e.g., db.t3, db.m5, db.r5).
*   **Storage:** General Purpose SSD (gp2/gp3), Provisioned IOPS SSD (io1/io2). Aurora uses a custom, auto-scaling storage layer.
*   **Multi-AZ Deployment:** Creates a synchronous standby replica in a different Availability Zone for high availability and failover.
*   **Read Replicas:** Asynchronous replicas to scale read traffic. Can be in the same region, cross-region, or even cross-account (Aurora).
*   **Database Engines:** Select the appropriate engine (MySQL, PostgreSQL, MariaDB, SQL Server, Oracle). Aurora is compatible with MySQL and PostgreSQL.
*   **Security Groups:** Control network access to the database instance.
*   **Parameter Groups:** Configure database engine parameters.
*   **Backup and Recovery:** Automated backups, manual snapshots. Point-in-time recovery. Aurora uses continuous backups to S3.
*   **Data Modeling (General Relational):**
    *   **Normalization:** Reduce data redundancy and improve data integrity (e.g., 3NF).
    *   **Denormalization:** Introduce redundancy strategically to improve read performance (common in data warehousing).
    *   **Indexing:** Create indexes on columns frequently used in `WHERE` clauses and `JOIN` conditions to speed up queries.

### General Implementation Concepts

*   **Partitioning/Sharding:** Dividing large tables/datasets into smaller, more manageable pieces.
    *   **Horizontal Partitioning (Sharding):** Splitting rows into multiple tables/databases based on a shard key (common in NoSQL and large relational systems).
    *   **Vertical Partitioning:** Splitting columns into multiple tables.
    *   *AWS Examples:* DynamoDB partitions based on partition key; Redshift distribution keys; manual sharding in RDS.
*   **Indexing Strategies:** Choosing the right columns and types of indexes (B-tree, hash, GIN, etc.) based on query patterns. Over-indexing can slow down writes.
*   **Data Modeling Best Practices:** Design schemas that reflect access patterns and optimize for the chosen data store's strengths (e.g., avoid complex joins in DynamoDB, use distribution/sort keys in Redshift).

Effective implementation involves not just setting up the service, but configuring it optimally for the specific workload and applying appropriate data modeling and management techniques.