[<- 2.2 Implement data storage solutions](./2.2_Implement_data_storage_solutions.md) | [Table of Contents](../../README.md) | [3.1 Implement data quality checks ->](../3_Data_Operations_and_Monitoring/3.1_Implement_data_quality_checks.md)
---
# Domain 2: Data Store Management
## Task 2.3: Manage the data lifecycle

Data lifecycle management (DLM) involves policies and procedures for handling data throughout its lifespan, from creation or acquisition to archival or deletion. Effective DLM helps optimize storage costs, meet compliance requirements, and improve data governance.

### S3 Lifecycle Management

S3 provides powerful features for automating DLM for objects stored in buckets.

*   **Lifecycle Rules:** Configured at the bucket level, these rules define actions to take on objects based on their age or version history.
    *   **Transition Actions:** Move objects to different storage classes after a specified number of days since creation or since becoming a noncurrent version.
        *   *Common Transitions:*
            *   Standard -> Standard-IA (Infrequent Access)
            *   Standard/Standard-IA -> Intelligent-Tiering
            *   Standard/Standard-IA -> One Zone-IA
            *   Standard/Standard-IA/Intelligent-Tiering/One Zone-IA -> Glacier Instant Retrieval
            *   Standard/Standard-IA/Intelligent-Tiering/One Zone-IA/Glacier Instant Retrieval -> Glacier Flexible Retrieval
            *   Standard/Standard-IA/Intelligent-Tiering/One Zone-IA/Glacier Instant Retrieval/Glacier Flexible Retrieval -> Glacier Deep Archive
        *   *Considerations:* Minimum storage durations and retrieval costs/times for each class. Small objects ( < 128KB) may not be cost-effective to transition to IA/Glacier classes due to per-object overhead.
    *   **Expiration Actions:** Permanently delete objects after a specified number of days since creation or becoming noncurrent. Can also delete expired object delete markers or incomplete multipart uploads.
    *   **Rule Scope:** Rules can apply to the entire bucket or be filtered based on prefixes (folders), object tags, or object size.

*   **S3 Intelligent-Tiering:**
    *   An S3 storage class that automatically moves data between two access tiers (Frequent Access and Infrequent Access) based on changing access patterns, without performance impact or operational overhead.
    *   Optionally, can activate Archive Access tiers (Archive Access, Deep Archive Access) for further savings on data that becomes rarely accessed.
    *   Ideal for data with unknown or unpredictable access patterns. Incurs a small monthly monitoring and automation fee per object.

### S3 Glacier and Glacier Deep Archive

These are secure, durable, and extremely low-cost storage classes designed for data archiving and long-term backup.

*   **Use Cases:** Replacing tape libraries, meeting regulatory compliance (e.g., HIPAA, SEC Rule 17a-4), archiving media assets, long-term database backups.
*   **Retrieval Options:**
    *   **Glacier Instant Retrieval:** Millisecond access, slightly higher cost than Flexible. Good for archives needing immediate access occasionally.
    *   **Glacier Flexible Retrieval (Formerly Glacier):**
        *   *Expedited:* 1-5 minutes (highest cost retrieval).
        *   *Standard:* 3-5 hours (default).
        *   *Bulk:* 5-12 hours (lowest cost retrieval, good for large amounts of data).
    *   **Glacier Deep Archive:** Lowest cost storage in the cloud.
        *   *Standard:* Within 12 hours.
        *   *Bulk:* Within 48 hours.
*   **Vaults:** Containers for storing archives in Glacier.
*   **Vault Lock:** Implement compliance controls (e.g., Write Once Read Many - WORM) via a lockable policy. Once locked, the policy is immutable.

### AWS Backup

A centralized service to manage backups across various AWS services.

*   **Supported Services:** EC2 instances, EBS volumes, RDS databases, Aurora clusters, DynamoDB tables, EFS file systems, FSx, Storage Gateway volumes, Neptune databases, DocumentDB clusters.
*   **Backup Plans:** Define backup frequency, retention period, backup window, and lifecycle rules (e.g., transition backups to cold storage like Glacier after a certain period).
*   **Backup Vaults:** Securely store backups. Vault Lock can be applied for WORM compliance.
*   **Cross-Region & Cross-Account Backup:** Copy backups to other regions or accounts for disaster recovery and compliance.
*   **Centralized Management:** Simplifies backup administration compared to managing backups individually for each service.

### Managing Data Lifecycle in Databases

*   **DynamoDB Time To Live (TTL):**
    *   Define a specific attribute representing an expiration timestamp (Unix epoch time).
    *   DynamoDB automatically deletes items whose TTL timestamp has passed, typically within 48 hours, at no extra cost.
    *   Useful for session data, logs, temporary records. Deleted items can optionally be sent to a DynamoDB Stream.
*   **RDS/Aurora Archival:**
    *   No built-in automatic TTL like DynamoDB.
    *   Requires custom application logic or scheduled jobs/scripts.
    *   **Strategies:**
        1.  **Periodic Deletion:** Run `DELETE` statements based on a timestamp column (can be resource-intensive on large tables).
        2.  **Partitioning (if supported by engine):** Partition tables by date (e.g., monthly partitions). Older partitions can be detached or dropped efficiently. PostgreSQL and Aurora PostgreSQL support partitioning.
        3.  **Export and Delete:** Export older data to S3 (e.g., using `SELECT INTO OUTFILE S3` in Aurora MySQL, or AWS Glue/DMS) and then delete it from the database. Data in S3 can then be managed by S3 Lifecycle policies and potentially queried via Athena or Redshift Spectrum.
*   **Redshift Archival:**
    *   Similar strategies to RDS/Aurora.
    *   Often involves unloading older data to S3 using the `UNLOAD` command.
    *   Data in S3 can be queried via Redshift Spectrum if needed.
    *   Table partitioning is not directly supported, but clustering keys (sort keys) on date columns help manage data.

### Key Considerations for DLM

*   **Compliance Requirements:** Understand data retention regulations specific to your industry (e.g., GDPR, HIPAA, financial regulations).
*   **Recovery Point Objective (RPO) / Recovery Time Objective (RTO):** Define how much data loss is acceptable and how quickly data must be recovered. This influences backup frequency and storage choices.
*   **Cost Optimization:** Balance storage costs with retrieval time/cost requirements. Don't archive data needed frequently; don't keep rarely accessed data in expensive storage.
*   **Data Access Patterns:** How frequently is older data accessed? Does it need to be instantly available or can retrieval take hours/days?
*   **Automation:** Leverage AWS services like S3 Lifecycle, Intelligent-Tiering, and AWS Backup to automate DLM processes, reducing manual effort and risk.

Managing the data lifecycle effectively is essential for controlling costs, meeting compliance mandates, and ensuring data remains available and useful according to business needs throughout its existence.